<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yuxin Chen - Wharton Statistics and Data Science</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yuxin Chen</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="Education.html">Experiences</a></div>
<div class="menu-item"><a href="Group.html">Group</a></div>
<div class="menu-item"><a href="Teaching.html">Teaching</a></div>
<div class="menu-item"><a href="Publications_year.html">Publications</a></div>
<div class="menu-item"><a href="Software.html">Software</a></div>
<div class="menu-item"><a href="Resume_Yuxin_CHEN.pdf">Resume</a></div>
<div class="menu-item"><a href="InvitedTalk.html">Talks</a></div>
<div class="menu-item"><a href="Workshops.html">Workshops</a></div>
</td>
<td id="layout-content">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y92NF7Q0DZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y92NF7Q0DZ');
</script>

<p><br /></p>
<h1>Yuxin Chen</h1>

<table class="imgtable"><tr><td>
<img src="images/photo_Dec2019.jpg" alt="alt text" width="200 px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>I am a Professor of <a href="https://statistics.wharton.upenn.edu/"> Statistics and Data Science</a> 
and of <a href="https://www.ese.upenn.edu//"> Electrical and Systems Engineering</a> at 
 University of Pennsylvania. 
Before moving to UPenn in 2022,   
I was an assistant professor of <a href="http://ece.princeton.edu/">Electrical and Computer Engineering</a>, and an associated faculty member of<a href="https://www.cs.princeton.edu/"> Computer Science</a> and <a href="https://www.pacm.princeton.edu/">Applied and Computational Mathematics</a> at Princeton University from 2017 to 2021. 
Prior to this, I was a postdoc in Statistics at Stanford University from 2015 to 2017, and obtained my Ph.D. in Electrical Engineering at Stanford University in Jan 2015. 
</p>
<p><b>Contact:</b> <br />
313 Academic Research Building <br />
265 South 37th Street, Philadelphia, PA 19104 <br /></p>
<p>Email: yuxinc at wharton dot upenn dot edu</p>
</td></tr></table>

<h2>Openings</h2>
<p>I'm looking for highly motivated postdocs and Ph. D. students with strong mathematical background and interest in machine learning theory (particularly diffusion models, LLM, and RL),   statistics, and optimization. </p>
<h2>Recent news</h2>

<table class="imgtable"><tr><td>
<p style="margin:3px;"></p>
<img src="images/spectral-methods-FnT.jpeg" alt="alt text" width="140 px" height="IMGLINKTARGET" />&nbsp;</td>
<td align="left"><p>
<ul>
    <li>Together with <a href="https://qingqu.engin.umich.edu/">Prof. Qing Qu</a> and <a href="https://liyueshen.engin.umich.edu/">Prof. Liyue Shen</a>, we presented a tutorial on diffusion models at ICML 2025. Check out <a href="https://icml-2025-tutorial.github.io/diffusion-models/">our website</a>. See also <a href="slides/Diffusion_theory_ICML_Chen.pdf">[slides of my part].</a> </li>
</ul>
<ul>
    <li>Together with <a href="https://yuejiechi.github.io/">Prof. Yuejie Chi</a> and <a href="https://yutingwei.github.io/">Prof. Yuting Wei</a>, we will present a tutorial on reinforcement learning at the <a href="https://meetings.informs.org/wordpress/annual/tutorials/">2025 INFORMS Annual Meetings.</a> See the <a href="https://arxiv.org/pdf/2507.14444">reading material</a>.  
    </li>
</ul>
<ul>
    <li><a href="https://link.springer.com/content/pdf/10.1007%2Fs10208-019-09429-9.pdf"> Our paper</a> on implicit regularization in nonconvex statistical estimation
 received the <a href="https://www.siam.org/prizes-recognition/activity-group-prizes/detail/siag-imaging-science-best-paper-prize"> SIAM Activity Group on Imaging Science Best Paper Prize</a> <a href="https://sinews.siam.org/Details-Page/may-prize-spotlight#Chen-et-al">(SIAM news)</a>. </li>
</ul>
<ul>
    <li><a href="https://ieeexplore.ieee.org/document/10232863/"> Our paper</a> on MagNet (an ML framework for power magnetics modeling)
 received the <a href="https://www.ieee-pels.org/awards/transactions-on-power-electronics-prize-paper-award/"> IEEE Transactions on Power Electronics Prize Paper Award (first place)</a>. <a href="https://www.princeton.edu/~minjie/magnet.html">(Project webpage by Prof. Minjie Chen's lab)</a>.</li>
</ul>
<ul>
    <li>Our new monograph <a href="publications/SpectralMethods.pdf"> "Spectral Methods for Data Science: A Statistical Perspective" </a> is published in <a href="https://www.nowpublishers.com/article/Details/MAL-079"><i> Foundations and Trends in Machine Learning</i></a>. See <a href="slides/spectral_method_slides.pdf"> slides </a> for PKU summer school on theoretical statistics.  </li>
</ul>
<br>
</p>
</td></tr></table>

<h2>Topic courses I have developed</h2>
<ul>
<li><p>Fall 2023: <a href="https://yuxinchen2020.github.io/large_scale_optimization/index.html">STAT9910-303: Large-Scale Optimization for Data Science</a> (UPenn)  <br /></p>
</li>
<li><p>Spring 2022: <a href="stat991_math_data/index.html">STAT991-302: Mathematics of High-Dimensional Data</a> (UPenn) <br /></p>
</li>
<li><p>Fall 2020: <a href="ele520_math_data/index.html">ELE520: Mathematics of Data Science</a> (Princeton) <br /> </p>
</li>
</ul>
<h2>Selected recent papers</h2>
<ul>
<li><p><b>Transformer theory</b>
</p>
<ul>
<li><p>Y. Huang*, Z. Wen, A. Singh, Y. Chi, <u>Y. Chen</u>, <a href="publications/transformers_CoT.pdf">&ldquo;Transformers provably learn chain-of-thought reasoning with length generalization,&rdquo;</a> 2025 (*=equal contributions).
</p>
</li>
<li><p>G. Li*, Y. Jiao*, Y. Huang, Y. Wei, <u>Y. Chen</u>, <a href="publications/TransformerICL.pdf">&ldquo;Transformers meet in-context learning: A universal approximation theory,&rdquo;</a> 2025 (*=equal contributions). <a href="slides/Transformer_ICL_slides.pdf">[slides]</a> </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Diffusion models</b>
</p>
<ul>
<li><p>J. Liang, Z. Huang, <u>Y. Chen</u>, <a href="publications/diffusion-low-dim.pdf">&ldquo;Low-dimensional adaptation of diffusion models: Convergence 
in total variation,&rdquo;</a> 2025 (accepted in part to COLT 2025).  <a href="slides/Diffusion_low_dim_slides.pdf">[slides]</a>
</p>
</li>
<li><p>Z. Huang, Y. Wei, <u>Y. Chen</u>, <a href="publications/DDPM-low-dim.pdf">&ldquo;Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality,&rdquo;</a> <i>Mathematics of Operations Research</i>, 2025+.
</p>
</li>
<li><p>G. Li, Y. Wei, Y. Chi, <u>Y. Chen</u>, <a href="publications/DiffusionODE.pdf">&ldquo;A sharp convergence theory for
the probability flow ODEs of diffusion models,&rdquo;</a> 2024. <a href="slides/Acc_diffusion_slides.pdf">[slides]</a>
</p>
</li>
<li><p>G. Li*, Y. Huang*, T. Efimov, Y. Wei, Y. Chi, <u>Y. Chen</u>, <a href="publications/AccDiffusion.pdf">&ldquo;Accelerating convergence of score-based diffusion models, provably,&rdquo;</a> ICML 2024 (*=equal contributions). <a href="slides/Acc_diffusion_slides.pdf">[slides]</a><a href="https://github.com/TimofeyEfimov/AcceleratedDiffusionSamplers">[code]</a>

</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Statistical learning</b>


</p>
<ul>
<li><p>Z. Zhang, W. Zhan, <u>Y. Chen</u>, S. S. Du, J. D. Lee, <a href="publications/MDL.pdf">&ldquo;Optimal multi-distribution learning,&rdquo;</a>  <i>Journal of the ACM</i>, vol. 72, no. 5, pp. 1-71, 2025 (appeared in part in COLT 2024). <a href="slides/MDL_slides.pdf">[slides]</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Reinforcement learning</b>
</p>
<ul>
<li><p>Z. Zhang, <u>Y. Chen</u>,  J. D. Lee, S. S. Du, <a href="publications/Optimal-OnlineRL.pdf">&ldquo;Settling the sample complexity of online reinforcement learning,&rdquo;</a> <i>Journal of the ACM</i>, vol. 72, no. 3, pp. 1-63, 2025 (appeared in part in COLT 2024). <a href="publications/Optimal-OnlineRL.pdf">[paper]</a><a href="https://dl.acm.org/doi/10.1145/3733592">[JACM version]</a><a href="slides/Optimal_OnlineRL_slides.pdf">[slides]</a>


</p>
</li>
<li><p>G. Li, L. Shi, <u>Y. Chen</u>, Y. Chi, Y. Wei,   <a href="publications/offline_modelbased.pdf">&ldquo;Settling the sample complexity of model-based offline reinforcement learning,&rdquo;</a> <i>Annals of Statistics</i>, vol. 52, no. 1, pp. 233-260, 2024. <a href="publications/offline_modelbased.pdf">[paper]</a><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-52/issue-1/Settling-the-sample-complexity-of-model-based-offline-reinforcement-learning/10.1214/23-AOS2342.full">[AoS version]</a>


</p>
</li>
<li><p>G. Li, Y. Wei, Y. Chi,  <u>Y. Chen</u>,  <a href="publications/SoftmaxPG_LB.pdf">&ldquo;Softmax policy gradient methods can take exponential time to converge,&rdquo;</a> <i>Mathematical Programming</i>, vol. 201, pp. 707-802, 2023 (appeared in part to COLT 2021). <a href="publications/SoftmaxPG_LB.pdf">[paper]</a><a href="https://link.springer.com/article/10.1007/s10107-022-01920-6">[MP version]</a><a href="slides/Softmax_Lower_Bound_slides.pdf">[slides]</a>
</p>
</li>
<li><p>G. Li, C. Cai, <u>Y. Chen</u>, Y. Wei, Y. Chi, <a href="publications/SyncQlearning.pdf">&ldquo;Is Q-Learning minimax optimal? A tight sample complexity analysis,&rdquo;</a> <i>Operations Research</i>, vol. 72, no. 1, pp. 203-221, 2024 (appeared in part to ICML 2021).  <a href="publications/SyncQlearning.pdf">[paper]</a><a href="https://pubsonline.informs.org/doi/epdf/10.1287/opre.2023.2450">[OR version]</a>
</p>
</li>
<li><p>S. Cen, C. Cheng, <u>Y. Chen</u>, Y. Wei, Y. Chi,  <a href="publications/NPG_reg.pdf">&ldquo;Fast global convergence of natural policy gradient methods with entropy regularization,&rdquo;</a> <i>Operations Research</i>, vol. 70, no. 4, pp. 2563–2578,  2022 <b>(INFORMS George Nicholson award finalist, 2021)</b>. <a href="publications/NPG_reg.pdf">[ArXiv]</a><a href="https://pubsonline.informs.org/doi/epdf/10.1287/opre.2021.2151">[OR version]</a><a href="slides/EntropyNPG_slides.pdf">[slides]</a>
</p>
</li>
<li><p>G. Li, Y. Wei, Y. Chi, <u>Y. Chen</u>, <a href="publications/model_based_RL.pdf">&ldquo;Breaking the sample size barrier in model-based reinforcement learning with a generative model,&rdquo;</a> <i>Operations Research</i>, vol. 72, no. 1, pp. 222-236, 2024 (appeared in part to NeurIPS 2020). <a href="publications/model_based_RL.pdf">[paper]</a><a href="https://pubsonline.informs.org/doi/epdf/10.1287/opre.2023.2451">[OR version]</a><a href="http://www.stat.cmu.edu/~ytwei/documents/slides/model-based-rl-slides.pdf">[slides]</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Nonconvex optimization for statistical estimation</b>
</p>
<ul>
<li><p><u>Y. Chen</u>, J. Fan, C. Ma, Y. Yan,  <a href="publications/RPCA_noise.pdf">&ldquo;Bridging convex and nonconvex optimization in robust PCA: Noise, outliers, and missing data,&rdquo;</a> <i>Annals of Statistics</i>, vol. 49, no. 5, pp. 2948-2971, Oct. 2021. <a href="publications/RPCA_noise.pdf">[ArXiv]</a><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-5/Bridging-convex-and-nonconvex-optimization-in-robust-PCA--Noise/10.1214/21-AOS2066.full">[AoS version]</a>




</p>
</li>
<li><p>C. Cai, G. Li, H. V. Poor, <u>Y. Chen</u>, <a href="publications/NonconvexTC.pdf">&ldquo;Nonconvex low-rank tensor completion from noisy data,&rdquo;</a> <i>Operations Research</i>, vol. 70, no. 2, pp. 1219–1237, 2022 (appeared in part in NeurIPS 2019). <a href="publications/NonconvexTC.pdf">[ArXiv]</a><a href="https://pubsonline.informs.org/doi/pdf/10.1287/opre.2021.2106">[OR version]</a><a href="slides/slides_TC.pdf">[slides]</a>
</p>
</li>
<li><p><u>Y. Chen</u>, J. Fan, C. Ma, Y. Yan,  <a href="https://www.pnas.org/content/pnas/116/46/22931.full.pdf">&ldquo;Inference and uncertainty quantification for noisy matrix completion,&rdquo;</a> <i>Proceedings of the National Academy of Sciences (PNAS)</i>, vol. 116, no. 46, pp. 22931–22937, Nov. 2019. <a href="publications/MC_inference.pdf">[ArXiv]</a><a href="https://www.pnas.org/content/pnas/early/2019/10/29/1910053116.full.pdf">[PNAS version]</a><a href="slides/NoisyMC_Inference_slides.pdf">[slides]</a>
</p>
</li>
<li><p><u>Y. Chen</u>, Y. Chi, J. Fan, C. Ma, Y. Yan,  <a href="https://epubs.siam.org/doi/pdf/10.1137/19M1290000">&ldquo;Noisy matrix completion: Understanding statistical guarantees for convex relaxation via nonconvex optimization,&rdquo;</a>  <i>SIAM Journal on Optimization</i>, vol. 30, no. 4, pp. 3098–3121, October 2020. <a href="publications/NoisyMC.pdf">[ArXiv]</a><a href="slides/NoisyMC_slides.pdf">[slides]</a>
</p>
</li>
<li><p><u>Y. Chen</u>, Y. Chi, J. Fan, C. Ma, <a href="https://link.springer.com/article/10.1007/s10107-019-01363-6">&ldquo;Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval,&rdquo;</a> <i>Mathematical Programming</i>, vol. 176, no. 1-2, pp. 5-37, July 2019.   <a href="publications/random_init_PR.pdf">[ArXiv]</a><a href="slides/random_init_slides.pdf">[slides]</a>
</p>
</li>
<li><p>C. Ma, K. Wang, Y. Chi, <u>Y. Chen</u>, <a href="https://link.springer.com/content/pdf/10.1007%2Fs10208-019-09429-9.pdf">&ldquo;Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and blind deconvolution,&rdquo;</a> <i>Foundations of Computational Mathematics</i>, vol. 20, no. 3, pp. 451-632, June 2020 
<b>(SIAM Activity Group on Imaging Science Best Paper Prize)</b>.  <a href="publications/ImplicitReg_main.pdf">[main text]</a><a href="publications/ImplicitReg_supp.pdf">[supplement]</a><a href="publications/ImplicitReg.pdf">[full paper (Arxiv)]</a><a href="slides/implicit_reg_slides.pdf">[slides]</a>


</p>
</li>
<li><p><u>Y. Chen</u> and E. J. Candes, <a href="publications/TruncatedWF_CPAM.pdf">&ldquo;Solving random quadratic systems of equations is nearly as easy as solving linear systems,&rdquo;</a> <i>Communications on Pure and Applied Mathematics</i>, vol. 70, issue 5, pp. 822-883, May 2017 <b>(ICCM Best Paper Award (Gold Medal), and Finalist for Best Paper Prize for Young Researchers in Continuous Optimization)</b>. <a href="slides/TWF_slides.pdf">[slides]</a><a href="http://princeton.edu/~yc5/TWF/">[website]</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Spectral methods</b>
</p>
<ul>
<li><p>Y. Zhou, <u>Y. Chen</u>, <a href="publications/deflated_heteroPCA.pdf">&ldquo;Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA,&rdquo;</a> <i>Annals of Statistics</i>, vol. 53, no. 1, pp. 91-116, 2025. <a href="publications/deflated_heteroPCA.pdf">[ArXiv]</a><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-53/issue-1/Deflated-HeteroPCA--Overcoming-the-curse-of-ill-conditioning-in/10.1214/24-AOS2456.full">[AoS version]</a><a href="slides/Deflated_HeteroPCA_slides.pdf">[slides]</a>
</p>
</li>
<li><p>Y. Yan, <u>Y. Chen</u>, J. Fan, <a href="publications/Denoising_PCA_inference.pdf">&ldquo;Inference for heteroskedastic PCA with missing data,&rdquo;</a> <i>Annals of Statistics</i>, vol. 52, no. 2, pp. 729-756, 2024. <a href="publications/Denoising_PCA_inference.pdf">[ArXiv]</a><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-52/issue-2/Inference-for-heteroskedastic-PCA-with-missing-data/10.1214/24-AOS2366.full">[AoS version]</a><a href="slides/PCA_Inference_short.pdf">[slides]</a>




</p>
</li>
<li><p>C. Cai, G. Li, Y. Chi, H. V. Poor, <u>Y. Chen</u>, <a href="publications/unbalanced_PCA.pdf">&ldquo;Subspace estimation from unbalanced and incomplete data matrices, $\ell_{2,\infty}$ statistical guarantees &rdquo;</a>  <i>Annals of Statistics</i>, vol. 49, no. 2, pp. 944-967, 2021. <a href="publications/unbalanced_PCA.pdf">[ArXiv]</a><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-2/Subspace-estimation-from-unbalanced-and-incomplete-data-matrices--%e2%84%932/10.1214/20-AOS1986.full">[AoS version]</a>


</p>
</li>
<li><p><u>Y. Chen</u>, J. Fan, C. Ma, K. Wang, <a href="publications/topK_AOS.pdf">&ldquo;Spectral method and regularized MLE are both optimal for top-<i>K</i> ranking,&rdquo;</a> <i>Annals of Statistics</i>, vol. 47, no. 4, pp. 2204-2235, August 2019. <a href="publications/topK.pdf">[ArXiv]</a><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-4/Spectral-method-and-regularized-MLE-are-both-optimal-for-top/10.1214/18-AOS1745.full">[AoS version]</a><a href="slides/topK_slides.pdf">[slides]</a></p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2026-02-18 00:15:14 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="index.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
